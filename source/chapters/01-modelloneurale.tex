%!TEX root = ../main.tex

\chapter{Modello Neurale}
\label{cha:modello_neurale}

Le reti neurali artificiali sono state create per riprodurre attività tipiche del cervello umano quali la percezione di immagini, il riconoscimento di forme, la comprensione del linguaggio e il coordinamento senso-motorio. A tale scopo sono state studiate le caratteristiche del cervello umano.								

Nel sistema nervoso esistono miliardi di neuroni (cellule nervose). Nel modello biologico un \textbf{neurone} è caratterizzato da:
\begin{itemize}
	\item \textbf{corpo cellulare (soma)}: l'unità di calcolo del neurone (5/10 micron);
	\item \textbf{assone}: meccanismo di output di un neurone;
	\item \textbf{dendriti}: ricevono segnali in input da altri assoni tramite le \textbf{sinapsi}.
\end{itemize}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/neuron.png}
	\caption{Un neurone biologico.}\label{fig:neuron}
\end{figure}
All'estremità l'assone si ramifica formando terminali attraverso i quali i segnali elettrici vengono trasmessi ad altre cellule (ad esempio ai dendriti di altri neuroni). Tra un terminale di un assone e la cellula ricevente esiste uno spazio: i segnali superano questo spazio per mezzo di sostanze chimiche dette \emph{neurotrasmettitori}. Il punto di connessione tra terminale e dendrite è detto \textbf{sinapsi}.

La trasmissione di un segnale nella corteccia cerebrale è un processo complesso.
In maniera molto semplificata, è composto delle seguenti fasi:
\begin{enumerate}
	\item il corpo cellulare esegue una somma pesata dei segnali in ingresso;
	\item se il risultato supera un certo valore soglia allora si produce un \textbf{potenziale d'azione}, ovvero una scarica di impulsi elettrici che viene inviata all'assone. In questo caso si dice che il neurone ha \emph{sparato}, altrimenti è rimasto inattivo;
	\item quando il segnale elettrico raggiunge la sinapsi, viene rilasciato chimicamente un \emph{neurotrasmettitore} che si combinerà con i \emph{recettori} nella membrana post-sinaptica;
	\item i recettori post-sinaptici provocano una diffusione del segnale elettrico nel neurone post-sinaptico.
\end{enumerate}
\begin{figure}[h!]
	\centering
	\includegraphics[width=8cm]{images/synapse.png}
	\caption{La sinapsi.}\label{fig:synapse}
\end{figure}
L'\textbf{apprendimento} si attua modificando la cosiddetta \textbf{efficacia sinaptica}, ovvero l'ammontare di corrente che entra nel neurone post-sinaptico rispetto al potenziale d'azione del neurone pre-sinaptico. In questo contesto le sinapsi possono essere \textbf{eccitatorie}, nel senso che favoriscono la generazione di potenziale d'azione nel neurone post-sinaptico, oppure \textbf{inibitorie}, se ne limitano la generazione.

Nel cervello non esiste un controllo centralizzato, nel senso che le varie zone del cervello funzionano insieme, influenzandosi reciprocamente e contribuendo alla realizzazione di uno specifico compito.

Infine, il cervello è \emph{fault tolerant}, ovvero se un neurone o una delle sue connessioni sono danneggiati, il cervello continua a funzionare, anche se con prestazioni leggermente degradate. In particolare, le prestazioni del processo celebrale degradano gradualmente man mano che si distruggono sempre più neuroni (\emph{graceful degradation}).

Per riprodurre artificialmente il cervello umano occorre realizzare una rete di elementi molto semplici che sia una struttura \textbf{distribuita}, massicciamente \textbf{parallela}, capace di \textbf{apprendere} e quindi di \textbf{generalizzare} (cioè produrre uscite in corrispondenza di ingressi non incontrati durante l'addestramento).

Il metodo più usato per addestrare una rete neurale consiste nel presentare in ingresso alla rete un insieme di esempi (training set). La risposta fornita dalla rete per ogni esempio viene confrontata con la risposta desiderata, si valuta la differenza (errore) fra le due e, in base ad essa, si aggiustano i pesi. Questo processo viene ripetuto sull'intero training set finché le uscite della rete producono un errore al di sotto di una soglia prestabilita.

Anche se di recente introduzione, le reti neurali trovano valida applicazione in settori quali predizione, classificazione, riconoscimento e controllo, portando spesso contributi significativi alla soluzione di problemi difficilmente trattabili con metodologie classiche.

\section{Modello di McCulloch \& Pitts}
\label{sec:modello_di_mcculloch_e_pitts}

Nel 1943, W. McCulloch e W. Pitts hanno presentato un modello artificiale del neurone biologico, avente molti ingressi ed una sola uscita: ciascun ingresso ha associato un peso, che determina la conducibilità del canale di ingresso. L'attivazione del neurone è una funzione della somma pesata degli ingressi.
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[shorten >=1pt, node distance=\layersep]
		% Draw the input layer nodes
		\foreach \name / \y in {1,...,3}
		% This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
		\node[input neuron, pin=left:$x_\y$] (I-\name) at (0,-\y) {};
		\node[input neuron,  pin=left:$x_n$] (I-4) at (0,-5) {};
		\draw[dotted, thick] (I-3) -- (I-4);
		% Draw the output layer node
		\node[output neuron, right of=I-3] (O) {$\Sigma$};
		\node[neuron, rectangle, right of=O, inner sep=5pt, pin={[pin edge={<-}]above:$\mu_i$}] (F) {$\Theta(x)$};
		\node[right of=F] (Y) {Output};
		\path (O) edge [->] (F)
		(F) edge [->] (Y);
		% Connect every node in the hidden layer with the output layer
		\foreach \source in {1,...,3}
		\path (I-\source) edge [->, above] node {\scriptsize$w_{i\source}$} (O);
		\path (I-4) edge [->] node [above=0.3cm of I-4] {\scriptsize$w_{in}$} (O);
		\node[above=0.5cm of I-1] (In) {Input layer};
		\node[right=\layersep of In] {Output layer};
	\end{tikzpicture}
	\caption{Neurone artificiale nel modello McCulloch \& Pitts.}
\end{figure}

\noindent Un insieme di neuroni artificiali forma una \textbf{rete neurale}: essa può essere rappresentata mediante un grafo diretto pesato $N=(V, E, w)$ dove $V$ è l'insieme delle unità (o neuroni), $E \subseteq V \times V$ è l'insieme di connessioni e $w:E \rightarrow \mathbb{R}$ è una funzione che assegna un peso con valore reale $w(i,j)$ per ogni connessione $(j, i)\in E$. I pesi rappresentano l'efficacia sinaptica: pesi positivi amplificano il segnale, mentre quelli negativi lo inibiscono. Come notazione sarà utilizzata $w_{ij}$ anziché $w(i,j)$.

Un neurone si attiva se la somma pesata degli input supera un certo valore soglia $\mu_i$\begin{align}
	x_i = \Theta\left(\sum_j w_{ij} x_j - \mu_i \right)
\end{align}
dove $\Theta(x)$ è la \textbf{funzione di attivazione} e può essere \emph{discreta}, come
\begin{align}
	x_i = \begin{cases}
		1 & \mbox{se } \displaystyle\sum_j w_{ij} x_j \geq \mu_i \\
		0 & \mbox{altrimenti} 
	\end{cases}
\end{align}
oppure \emph{continua}, in modo tale da rendere il sistema più realistico: le funzioni continue più utilizzate sono la \emph{sigmoidea} e la \emph{tangente iperbolica}, in quanto permettono di ridurre la varianza in output di un neurone mappando il risultato della somma pesata entro un intervallo ridotto, $[0, 1]$ nel caso della prima e $[-1, 1]$ nel caso della seconda.
\begin{figure}[h!]
	\begin{center}
		\subfigure[Tangente iperbolica]{
		\begin{tikzpicture}[domain=-1:1,scale=1.5]
			\draw[->] (-1,0) -- (1,0) node[right] {\tiny$x$};
			\draw[->] (0,-1.2) -- (0,1.2) node[above] {\tiny$f(x)$};
			\draw[dashed, thin] plot function{1} node[above]{\tiny +1};
			\draw[dashed, thin] plot function{-1} node[above]{\tiny-1};
			\draw[color=blue, thick] plot[id=tanh] function{tanh(x)};
		\end{tikzpicture}}
		\qquad
		\subfigure[Sigmoidea]{
		\begin{tikzpicture}[domain=-1:1,scale=1.5]
			\draw[->] (-1,0) -- (1,0) node[right] {\tiny$x$};
			\draw[->] (0,-1.2) -- (0,1.2) node[above] {\tiny$f(x)$};
			\draw[dashed, thin] plot function{1} node[above]{\tiny +1};
			\draw[color=orange, thick] plot[id=sigmoidal] function{1 / (1 +  exp(- 10 * x))};
		\end{tikzpicture}}
		\caption{Le funzioni di attivazione continue più utilizzate.}
	\end{center}
\end{figure}

\noindent È possibile sbarazzarsi dei valori soglia associati ai neuroni aggiungendo una nuova unità extra permanentemente bloccata a -1. In questo modo i valori soglia diventano pesi e possono essere opportunamente aggiustati durante l'apprendimento.

\medskip

Combinando opportunamente i neuroni di McCulloch \& Pitts si possono costruire reti in grado di realizzare qualsiasi operazione del calcolo proposizionale. È importante notare che in questo modello \textbf{non} avviene ancora alcun tipo di apprendimento: i pesi sono fissi, mentre l'apprendimento si realizza attraverso la variazione dei pesi sinaptici.
\begin{figure}[h!]
	\centering

	\subfigure[AND]{
	\begin{tikzpicture}[node distance=\layersep]]
		\node[input neuron] (I-1) at (0,1) {1};
		\node[input neuron] (I-2) at (0,3) {0};
		\node[output neuron, pin={[pin edge={<-}]above:$\mu=\frac{3}{2}$}, right of=I] (S) at (0,2) {1};
		\node[right of=S] (O) {0};
		\path 
		(I-1) edge[->] node[above] {1} (S)
		(I-2) edge[->] node[above] {1} (S)
		(S) edge[->] node[above] {$1 < \frac{3}{2}$} (O);
	\end{tikzpicture}}
	\qquad
	\subfigure[OR]{
	\begin{tikzpicture}[node distance=\layersep]]
		\node[input neuron] (I-1) at (0,1) {1};
		\node[input neuron] (I-2) at (0,3) {0};
		\node[output neuron, pin={[pin edge={<-}]above:$\mu=\frac{1}{2}$}, right of=I] (S) at (0,2) {1};
		\node[right of=S] (O) {1};
		\path 
		(I-1) edge[->] node[above] {1} (S)
		(I-2) edge[->] node[above] {1} (S)
		(S) edge[->] node[above] {$1 \geq \frac{1}{2}$} (O);
	\end{tikzpicture}}
	\subfigure[NOT]{
	\begin{tikzpicture}[node distance=\layersep]]
        
		\node[input neuron] (I) {1};
		\node[output neuron, pin={[pin edge={<-}]above:$\mu=-\frac{1}{2}$}, right of=I] (S) {-1};
		\node[right of=S] (O) {0};
		\path 
		(I) edge[->] node[above] {-1} (S)
		(S) edge[->] node[above] {$-1 < -\frac{1}{2}$} (O);
	\end{tikzpicture}}
	\caption{Operazioni logiche elementari nel modello M\&P.}
\end{figure}

\newpage

\section{Tipi di architetture della rete}
\label{sec:tipi_di_architetture_della_rete}

In generale le reti neurali vengono classificate sulla base di tre caratteristiche:
\begin{itemize}
	\item \textbf{presenza di cicli}: reti \emph{feedforward} (o acicliche) e reti \emph{ricorrenti} (o cicliche);
	\item \textbf{connettività}: reti fortemente o scarsamente connesse;
	\item \textbf{numero di strati}: reti \emph{single layer} e reti \emph{multilayer}.
\end{itemize}
La scelta della rete neurale da utilizzare dipende dalla tipologia di problema che si vuole risolvere. In generale si identificano tre classi di reti:
\begin{itemize}
	\item \textbf{reti feedforward ad uno strato}: reti di questo tipo contengono i nodi di input (\emph{input layer}) e uno strato di neuroni (\emph{output layer}). Il segnale nella rete si propaga in avanti in modo aciclico, partendo dallo strato di input ed arrivando in quello di output;
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}[->, node distance=\layersep]

			% Draw the input layer nodes
			\foreach \name / \y in {1,...,3}
			\node[input neuron] (I-\name) at (0,-\y) {};

			% Draw the output layer nodes
			\foreach \name / \y in {1,...,2}   
			\node[output neuron] (O-\name) at (\layersep, -\y cm - 0.5cm) {};

			% Connect every node in the input layer with every node in the output layer.
			\foreach \source in {1,...,3}
			\foreach \dest in {1,...,2}
			\path (I-\source) edge (O-\dest);
                    
			% Annotate the layers
			\node[annot,above of=I-1, node distance=1cm] (il) {Input layer};
			\node[annot,right of=il] {Output layer};
		\end{tikzpicture}
		\caption{Rete feedforward ad uno strato.}
	\end{figure}
	\item \textbf{reti feedforward a più strati}: in questo caso, tra lo strato di input e quello di output, sono presenti uno o più strati di neuroni nascosti (\textbf{hidden layers}). Questo tipo di architettura fornisce alla rete una prospettiva globale in quanto aumentano le interazioni tra neuroni;
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}[->, node distance=\layersep]

			% Draw the input layer nodes
			\foreach \name / \y in {1,...,4}
			\node[input neuron] (I-\name) at (0,-\y) {};

			% Draw the hidden layer nodes
			\foreach \name / \y in {1,...,3}
			\path[yshift=2cm]
			node[hidden neuron] (H-\name) at (\layersep, -\y cm - 2.5 cm) {};

			% Draw the output layer nodes
			\foreach \name / \y in {1,...,2}   
			\node[output neuron] (O-\name) at (2*\layersep, -\y cm - 1cm) {};

			% Connect every node in the input layer with every node in the hidden layer.
			\foreach \source in {1,...,4}
			\foreach \dest in {1,...,3}
			\path (I-\source) edge (H-\dest);
                    
			% Connect every node in the hidden layer with every node in the ouput layer.
			\foreach \source in {1,...,3}
			\foreach \dest in {1,...,2}
			\path (H-\source) edge (O-\dest);
                    
			% Annotate the layers
			\node[annot,above of=H-1, node distance=1.5cm] (hl) {Hidden layer};
			\node[annot,left of=hl] {Input layer};
			\node[annot,right of=hl] {Output layer};
		\end{tikzpicture}
		\caption{Rete feedforward a più strati.}
	\end{figure}
	\item \textbf{reti ricorrenti}: reti di questo tipo contengono cicli, la cui presenza ha un impatto profondo sulle capacità di apprendimento della rete e sulle sue performance, in quanto la rendono un sistema \textbf{dinamico}.    
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}[->, node distance=\layersep]

			% Draw the input layer nodes
			\foreach \name / \y in {1,...,3}
			\node[input neuron] (I-\name) at (0,-\y) {};

			% Draw the output layer nodes
			\foreach \name / \y in {1,...,2}   
			\node[output neuron] (O-\name) at (\layersep, -\y cm - 0.5cm) {};

			% Connect every node in the input layer with every node in the output layer.
			\foreach \source in {1,...,3}
			\foreach \dest in {1,...,2}
			\path (I-\source) edge (O-\dest);
                    
			% Connect every node in the output layer with every node in the ouptu layer.
			\foreach \source in {1,...,2}
			\foreach \dest in {1,...,2} 
			{
			\ifnum \source < \dest
			\path (O-\source) edge [bend left] (O-\dest);
			\fi
			\ifnum \source > \dest
			\path (O-\source) edge [bend left] (O-\dest);
			\fi
			}
			% Annotate the layers
			\node[annot,above of=I-1, node distance=1cm] (il) {Input layer};
			\node[annot,right of=il] {Output layer};
		\end{tikzpicture}
		\caption{Rete ricorrente.}
	\end{figure}
    
\end{itemize}
