%!TEX root = /Users/andrea/Unive/Specialistica/Artificial Intelligence/Mod. A/Dispensa/main.tex
\chapter{Clustering} % (fold)
\label{cha:clustering}

Il clustering può essere considerato il più importante problema di \textbf{apprendimento non supervisionato} in quanto trova innumerevoli applicazioni in svariati campi del sapere. L’obiettivo che si pone è organizzare dati non classificati in gruppi, i cui membri sono simili per un qualche criterio. Un \textbf{cluster} è quindi una collezione di oggetti che sono simili tra di loro, e dissimili dagli oggetti appartenenti ad altri cluster. Formalmente:

\begin{mydef}[Problema del clustering]
	Dati $n$ oggetti e una matrice di similarità $n \times n$ lo scopo è partizionare gli inputs in gruppi massimalmente omogenei (i.e \textbf{clusters}).
\end{mydef}

Il criterio di similarità che deve essere fornito per poter fare un clustering dei dati può essere visto come una funzione $\phi$ che dati due oggetti ritorna la loro similarità. Questa misura è \emph{simmetrica} se per una qualunque coppia di oggetti $(a, b)$ abbiamo che $\phi(a, b) = \phi(b, a)$ altrimenti è \emph{asimmetrica}.\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=8cm]{images/clustering.png}
	\caption{Esempio di clustering}\label{fig:clusters}
\end{figure}

In questo esempio è possibile identificare tre cluster nei quali possono essere suddivisi i dati e il criterio di similarità usato è la \emph{distanza}, quindi due oggetti fanno parte di uno stesso cluster solo se sono sufficientemente vicini tra di loro. 

\newpage

Questo tipo di clustering è detto \emph{distance-based}, ovvero basato sulla distanza. In generale, a seconda del tipo di input il problema del partizionamento si divide in:
\begin{enumerate}
	\item \textbf{feature-based:} gli oggetti sono rappresentati come vettori di caratteristiche. Nella letteratura l'algoritmo più noto è il \emph{k-means};
	\item \textbf{pairwise:} le proprietà degli oggetti sono meglio descritte in termini di \emph{similarità/dissimilarità} tra di essi. In questo caso l'input è determinato da una matrice di affinità che indica la similarità tra tutte le coppie di oggetti. Si tratta quindi di un approccio più generale rispetto al primo.
\end{enumerate}

Non sempre è facile ed intuitivo trovare un raggruppamento per i dati, come invece lo era per l’esempio visto, in quanto è difficile stabilire cosa costituisce un “buon clustering” e cosa no. In generale si può affermare che un cluster deve soddisfare i seguenti criteri:
\begin{itemize}
	\item \textbf{criterio interno}: tutti gli oggetti all'\emph{interno} di un cluster devono essere il più possibile simili tra loro;
	\item \textbf{criterio esterno}: tutti gli oggetti all'\emph{esterno} di un cluster devono essre il più possibile dissimili rispetto a quelli contenuti al suo interno.
\end{itemize}

Nelle sezioni seguenti saranno presentati tre metodi di clustering:  K-Means, N-Cut e insiemi dominanti. Il primo è di tipo feature-based, mentre i rimanenti sono entrambi di tipo pairwise, ma utilizzano tecniche differenti.

\newpage

\section{K-Means} % (fold)
\label{sec:k_means}
K-Means è probabilmente il più famoso algoritmo di partizionamento. Dato un insieme di osservazioni $\{x^{(1)},\dots,x^{(n)} \}$ dove $x_i$ è un vettore m-dimensionale, lo scopo dell'algortimo è partizionare l'insieme in $k$ insiemi $k \leq n$ il più possibile coesi tra loro (problema del clustering).\\

L'algoritmo k-means seleziona a caso $k$ prototipi, ciascuno di essi rappresenta il proprio gruppo di appartenzenza, idealmente il centro del cluster. Formalmente si introduce un insieme di vettori:
\begin{align*}
	\mu_j, \text{ dove } j = 1, \dots, k
\end{align*}
L'obiettivo ora consiste nel raggruppare le osservazioni e assegnare i vettori $\mu_j$ in modo tale che la somma del quadrato delle distanze da un punto al proprio centroide $\mu_j$ sia minima. In termini matematici si tratta di minimizzare la seguente funzione:
\begin{align}
	\min_{\{\mu_1, \dots, \mu_k\}} \sum_{i=1}^n \sum_{j=i}^k r_{ij} \| x_i - \mu_j \|^2\label{eq:j}
\end{align}
dove $r_{ij}$ è un operatore binario $r_{ij} \in \{ 0, 1 \}$. In fase di inizializzazione, si scelgono $k$ punti casuali come centri e in seguito si esegue una procedura iterativa per minimizzare \eqref{eq:j}. Ogni iterazione si compone di \textbf{due} fasi: nella prima fase (\emph{Expectation-Step}) si minimizza rispetto a $r_{ij}$ mantenendo $\mu_j$ fisso, mentre nella seconda fase (\emph{Maximization-Step}) si minimizza rispetto a $\mu_j$ mantenendo $r_{ij}$ fisso. Formalmente, nella fase E, il calcolo di $r_{ij}$ è dato da:
\begin{align*}
	r_{ij} =
	\begin{cases}
		1, &\text{ se }j = argmin_j \| x_i - \mu_j \|^2 \\
		0, &\text{ altrimenti}
	\end{cases}
\end{align*}
Intuitivamente si assegna l’osservazione $x_i$ al gruppo con $\mu_j$ più vicino.

\newpage

Nella seconda fase $\mu_k$ è dato da:

\begin{align*}
	\mu_j = \frac{\displaystyle\sum_i r_{ij} x_i}{\displaystyle\sum_i r_{ij}}
\end{align*}

Ovvero, si cerca di “spostare” al centro del cluster il prototipo $\mu_j$ come la media di tutti i punti $x_i$ assegnati al cluster $k$. Queste due fasi si ripetono fino a quando non c'è più alcuna variazione negli assegnamenti o quando un numero massimo di iterazioni è raggiunto. Infatti, trattandosi di un problema NP-Difficile non è dato sapere a priori se l'algoritmo converga oppure no.

\begin{figure}[h!]
    \centering
	\subfigure{
    \includegraphics[width=12cm]{images/kmeans}
	}
	\subfigure{
    \includegraphics[width=12cm]{images/kmeans2}
	}
	\caption{Esempio sulla convergenza (dopo 5 iterazioni) a un minimo locale dell'algoritmo k-means}
\end{figure}

% section k_means (end)


\section{Normalized Cut} % (fold)
\label{sec:normalized_cut}
A differenza del k-means l'algoritmo presentato in questa sezione è basato sulla teoria dei grafi e trova ampio utilizzo nella \emph{segmentazione delle immagini}, un problema analogo al clustering.
L'algoritmo \emph{normalized cut} “taglia” l'immagine utilizzando tecniche della \textbf{teoria spettrale} dei grafi. Un buon taglio divide pixel che sono dissimili tra loro. Per trovare un buon partizionamento l'algoritmo segue i seguenti passi:
\begin{enumerate}
	\item Si costruisce un grafo di similarità $G=(V, E, w)$ in cui ogni nodo rappresenta un pixel dell'immagine e il peso su ciascun arco rappresenta una misura di similarità (intensità, colore ecc..) tra ciascun pixel.
	\item Si calola il Laplaciano normalizzato definito come:
	\begin{align}
		L = D ^ {- 1 / 2} (D - A) D ^ {- 1 / 2}
	\end{align}
	dove $A$ è la matrice di adiacenza pesata e $D$ è la matrice dei gradi così definita:
	\begin{align}
		d_{i,j}:=
		\begin{cases} 
		\deg(v_i) & \mbox{if}\ i = j \\
		0 & \mbox{otherwise}
		\end{cases}
	\end{align}
	Si calcolano gli autovettori del Laplaciano normalizzato:
	\begin{align}
		L x = \lambda x
	\end{align}
	
	\item Utilizzando il segno nel secondo autovettore più piccolo (il primo è un vettore nullo) si segmenta l'immagine in due parti.

	\item La procedura si ripete ricorsivamente fino a quando si ottiene il numero desideerato di segmenti.
\end{enumerate}

L'algoritmo nella pratica si è dimostrato abbastanza efficace tuttavia il calcolo degli autovettori è un problema computazionalmente costoso: $O(n^3)$ dove $n$ è il numero di pixel. Tuttavia è possibile velocizzare l'algoritmo sfruttando il fatto che la matrice è sparsa (i valori sono quasi tutti pari a zero) ed abbassare la complessità a $O(n \sqrt{n})$\\

\begin{figure}[h!]
    \centering
	\subfigure[$original$]
	{\includegraphics[width=3cm]{images/0.jpg}}
	\subfigure[$k = 2$]
	{\includegraphics[width=3cm]{images/1.png}}
	\subfigure[$k = 3$]
	{\includegraphics[width=3cm]{images/2.png}}
	\subfigure[$k = 4$]
	{\includegraphics[width=3cm]{images/3.png}}
	\caption{Esempio di segmentazione ottenuta con NCUT.}
\end{figure}


% section normalized_cut (end)

\newpage


\section{Insiemi dominanti} % (fold)

Il concetto di insieme dominante nasce dallo studio sulla formulazione continua del problema della cricca massima (vedi Sezione~\ref{sec:problema_della_cricca_massima}). L'insieme dominante, infatti, non è altro che una clique di un grafo nel caso in cui gli archi siano pesati, ovvero insiemi di vertici che hanno alta omogeneità interna e alta disomogeneità verso l'esterno.\\

In questo contesto i dati sono, dunque, rappresentati da un grafo non orientato $G = (V, E , w)$ dove $V = \{ 1, \dots, n \}$ è l'insieme dei vertici, $E\subseteq V \times V$ l'insieme degli archi e $w : E \rightarrow \mathbb{R}_+$ è la funzione dei pesi positiva. I vertici rappresentato i datapoints, gli archi le relazioni tra essi e i pesi rispecchiano la similarità tra coppie di vertici connessi.\\

Come di consueto, il grafo è rappresentato con la sua matrice di adiacenza pesata $A$, che è la matrice simmetrica $n \times n$ dove $A = a_{ij}$ è definito nel seguente modo:
\begin{align*}
	a_{ij} =
	\begin{cases}
		w(i, j), &\text{ if }(i, j) \in E\\
		0, &\text{ otherwise }
	\end{cases}
\end{align*}

Si consideri $S \subseteq V$ un insieme non vuoto di vertici $i \in S$. Si introduce il concetto di \textbf{grado pesato medio} di $i$ rispetto ad $S$ definito come:\\

\begin{minipage}{.5\textwidth}
	\begin{align*}
		awdeg_S(i) = \frac{1}{|S|}\sum_{j \in S} a_{ij}
	\end{align*}
	E nel caso in cui $j \not\in S$:
	\begin{align*}
		\phi_S(i,j) = a_{ij} - awdeg_S(i)
	\end{align*}
\end{minipage}
\begin{minipage}{.5\textwidth}
 	\begin{tikzpicture}[node distance=2cm]
 		\fill [gray!20] (-1, 0) ellipse  (2cm and 3cm);
 		\node at (-1, 3.4) {$S$};
 	    \node[Node] (1) at (0,0) {$i$};
 	    \node[Node, left of=1] (2) {};
 	    \node[Node, above of=2] (3) {};
 	    \node[Node, below of=2] (4) {};
 	    \node[Node, right=2cm of 1] (0) {$j$};
 	    \path
 		(1) edge (2) edge (3) edge (4) edge[above] node{$a_{ij}$} (0)
 		;
 	\end{tikzpicture}\\
\end{minipage}

Intuitivamente $\phi_S(i, j)$ misura la similarità tra il nodo $j$ e il nodo $i$ rispetto alla similarità media tra il nodo $i$ e i suoi vicini in $S$

\newpage

Il peso di $i$ rispetto ad $S$ è quindi definito come:
\begin{align*}
	w_S(i) =
	\begin{cases}
		1, &\text{ if }|S|=1\\
		\displaystyle\sum_{j \in S} \phi_{S \setminus \{i\}} (i, j) \cdot w_{S \setminus \{i\}}(j) &\text{ otherwise}
	\end{cases}
\end{align*}
Si tratta di una definizione ricorsiva e permette di assegnare un peso (similarità relativa) ad ogni vertice.
E il peso totale di $S$ è:

\begin{minipage}{.5\textwidth}
		\begin{align*}
			W(S) = \sum_{i \in S} w_S(i)
		\end{align*}
\end{minipage}
\begin{minipage}{.5\textwidth}
		\begin{tikzpicture}[node distance=2cm]
			\fill[gray!20] (0, 0) circle  (3cm);
			\fill[gray!40, rotate=-45] (-0.5, 0) ellipse  (2cm and 2.5cm);
			\node[Node] (1) at (0, 0) {$j$};
			\node[Node, above of=1] (2) {};
			\node[Node, below left of=1] (3) {};
			\node[Node, above of=3] (4) {};
		    \node[Node, below right=1.5cm of 1] (5) {$i$};
			\node[Node, above right of=1] (6) {};
		    \path (1) edge (2) edge (3) edge (5) edge (6)
			(3) edge (4)
			;
		
			\node (1) at (0, 3.4) {$S$};
			\node (1) at (0, -2.3) {$S \setminus \{i\}$};
		\end{tikzpicture}
		\\
\end{minipage}

Intuitivamente $w_S(i)$ ci dà la misura di similarità complessiva tra il nodo $i$ e i vertici $S \setminus \{i\}$ rispetto alla similarità complessiva di ciascun vertice in $S \setminus \{i\}$. Si considerino i seguenti grafi.

\begin{figure}[h!]
    \centering
	\subfigure[$W_{\{1,2,3,4\}(1)} < 0$]{
    \begin{tikzpicture}[node distance=3cm]
        \node[circle,fill=red!20] (1) at (0,1.5) {1};
        \node[circle,fill=blue!20, right of=1] (2) {2};
        \node[circle,fill=blue!20, below of=1] (4){4};
        \node[circle,fill=blue!20, below of=2] (3) {3};

        \foreach \from / \to in {1/2,1/3,1/4}
            \path (\from) edge[dashed, near start, auto] node{$1$} (\to);

        \foreach \from / \to in {4/2,4/3,3/2}
            \path (\from) edge[near start, auto] node{$10$} (\to);
    \end{tikzpicture}
	}
	\qquad\qquad
	\subfigure[$W_{\{5,6,7,8\}(5)} > 0$]{
    \begin{tikzpicture}[node distance=3cm]
        \node[circle,fill=red!20] (1) at (0,1.5) {5};
        \node[circle,fill=blue!20, right of=1] (2) {6};
        \node[circle,fill=blue!20, below of=1] (4){8};
        \node[circle,fill=blue!20, below of=2] (3) {7};

        \foreach \from / \to in {1/2,1/3,1/4}
            \path (\from) edge[dashed, near start, auto] node{$10$} (\to);

        \foreach \from / \to in {4/2,4/3,3/2}
            \path (\from) edge[near start, auto] node{$1$} (\to);
    \end{tikzpicture}
	}
    \caption{Rappresentazione della similarità: i nodi $\{2,3,4\}$ sono altamente simili tra loro. Se si tenta di aggiungere il vertice 1 che è dissimile dalla similarità interna dell'insieme $\{2,3,4\}$ allora la similarità complessiva del nuovo insieme $\{1,2,3,4\}$ diminuisce i.e. $W_{\{1,2,3,4\}}(1) < 0$. Nel secondo caso il vertice 5 è altamente similare ai nodi $\{6,7,8\}$ di conseguenza aggiungendolo all'insieme la similarità complessiva aumenta i.e. $W_{\{5,6,7,8\}}(5) > 0$.}
\end{figure}

\newpage

È stata dunque definita una misura che descrive cosa comporta (in termini di similarità) l'aggiunta o la rimozione di un nodo. Questo porta alla definizione di \emph{insiemi dominanti}.

\begin{mydef}[Insieme Dominante]
	Un sottoinsieme di vertici non vuoto $S \subseteq V$ tale che $W(T) > 0$ per ogni insieme non vuoto $T \subseteq S$ è detto \textbf{dominante} se:
	\begin{enumerate}
		\item $W_S(i) > 0, \forall i \in S$ (omogeneità interna)
		\item $W_{S \cup \{i\}}(i) < 0, \forall i \not\in S$ (disomogeneità esterna)
	\end{enumerate}
\end{mydef}
L'insieme dominante è dunque un insieme di vertici massimalmente coesi tra loro e questa definizione corrisponde con quella di cluster.

\begin{figure}[h!]
    \centering
	
    \begin{tikzpicture}[node distance=3cm]
        \node[circle,fill=blue!20] (1) at (0,0) {1};
		
        \node[circle,fill=blue!20, below left of=1] (2) {2};
        \node[circle,fill=blue!20, below right of=1] (3) {3};
        
        \node[circle,fill=blue!20, below of=2] (4) {4};
        \node[circle,fill=blue!20, below of=3] (5) {5};

        \foreach \from / \to / \label in {1/2/10,1/3/15,1/4/60,1/5/70}
            \path (\from) edge[near start] node{$\label$} (\to);
			
        \foreach \from / \to / \label in {2/4/25,2/5/20}
            \path (\from) edge[near start] node{$\label$} (\to);

        \foreach \from / \to / \label in {3/4/25,3/5/90}
            \path (\from) edge[near start] node{$\label$} (\to);

		\path (4) edge[auto] node{5} (5);
		\path (2) edge[auto] node{20} (3);

		\begin{pgfonlayer}{background}    % select the background layer
			\fill[gray!20] plot coordinates {(1) (3) (5)};
	    \end{pgfonlayer}

    \end{tikzpicture}
	\caption{L'insieme $\{1,3,5\}$ è dominante.}
\end{figure}

Quando la matrice di affinità è binaria 0/1 allora l'insieme dominante coincide con la cricca massimale (stretta) in un grafo (vedi Sezione~\ref{sec:problema_della_cricca_massima}).\\

\newpage

\subsection{Insiemi dominanti e ottimi locali} % (fold)
\label{sub:insiemi_dominanti_e_ottimi_locali}
In questa sezione sarà mostrato come la teoria di Nash possa essere utilizzata per risolvere in modo approssimato la ricerca di insieme dominanti.\\

Dato un grafo pesato $G = (V, E, w)$ e la sua matrice di adiacenza pesata $A$, si consideri il seguente programma quadratico standard:
\begin{align*}
	&\text{maximize } f(x) = x^T A x\\
	&\text{subject to } x \in \Delta
\end{align*}
dove $\Delta$ è il simplesso standard su $R^n$. Si introduce il seguente teorema:

\begin{thm}
	Se $S$ è un sottoinsieme dominante di vertici, allora il suo vettore caratteristico pesato $x^S$ definito come:
	\begin{align*}
		x^S_i = 
		\begin{cases}
			\frac{W_S(i)}{W(S)}, &\text{ se }i \in S\\
			0 &\text{ altrimenti}
		\end{cases}
	\end{align*}
	è un massimizzatore locale stretto di $f$ in $\Delta$.
	
	Al contrario se $x^*$ è un massimizzatore locale stretto di $f$ in $\Delta$ allora il suo supporto:
	\begin{align*}
		\sigma = \sigma(x^*) = \{ i \in V : x^*_i \neq 0 \}
	\end{align*}
	è un insieme dominante a condizione che $W_{\sigma \cup \{i\}} \neq 0$ per ogni $i \not\in \sigma$. 
\end{thm}
Dove la condizione $W_{\sigma \cup \{i\}} \neq 0$ è un tecnicismo dovuto alla presenza di soluzioni spurie, ovvero soluzioni che non ammettono vettore caratteristico pesato. Si tratta di una generalizzazione del teorema di Motzkin Strauss.\\

Ora che è stata fornita una caratterizzazione di insieme dominante è possibile utilizzare una qualunque tecnica di ottimizzazione quadratica per risolvere il problema (ad esempio la discesa del gradiente). Tuttavia le dinamiche di replicazione (vedi Sezione~\ref{sec:teoria_dei_giochi_evoluzionistici}) si sono rivelate particolarmente adatte per affrontare questo problema. È stato infatti dimostrato il seguente teorema:

\begin{thm}[Torsello, Rota Bulò, Pelillo 2006]
	Strategie evolutionary stable (ESS) di un problema di clustering con matrice di affinità $A$ sono in corrispondenza uno-a-uno con gli insiemi dominanti.
\end{thm}

\newpage

Quindi sia $A$ la matrice di adiacenza del grafo di similarità. Si pone:
\begin{align*}
	W = A (= W^T \geq 0)
\end{align*}
Allora un sistema di replicazione con matrice di payoff $W$ partendo da uno stato iniziale arbitrario convergerà, per il principio di selezione naturale, a un massimizzatore della funzione $f(x) = x^T A x$ nel simplesso standard. Questo corrisponderà a un insieme dominante in un grafo, ovvero ad un cluster di vertici. Quindi la ricerca di dominant sets di un grafo pesato e non orientato $G$, corrisponde a trovare equilibri di Nash asintoticamente stabili e le dinamiche di replicazione sono un ottimo strumento a disposizione per perseguire questo scopo.\\

La teoria dei giochi evoluzionistici opera in uno scenario in cui coppie di individui sono scelti ripetutamente a caso da una grande popolazione per competere in un gioco a due giocatori. A differenza della teoria dei giochi classica, i giocatori non si comportano razionalmente ma sono pre-programmati a un certo pattern comportamentale o una strategia mista. Col passare del tempo il principio di selezione naturale inciderà sulla distribuzione dei comportamenti.\\

Ad esempio si supponga di volere separare lo sfondo in un'immagine dagli elementi in primo piano. Nel gioco evoluzionistico ogni giocatore è pre-programmato a selezionare con una certa probabilità un elemento (un pixel) dall'immagine. In questo contesto la selezione naturale porterà giocatori che adottano strategie migliori (payoff più alto) ad espandersi, mentre quelli che adottano strategie peggiori ad estinguirsi. Nel caso descritto, ci si aspetta che la selezione naturale porti all'estinzione i giocatori che selezionano lo sfondo, per poi convergere a una popolazione che seleziona solo gli elementi in primo piano.\\

Concludendo le caratteristiche principali che rendono questo approccio preferibile rispetto ad altri sono le seguenti:
\begin{enumerate}
	\item Non richiede alcuna rappresentazione dei dati ovvero che gli elementi debbano essere rappresentato come punti in uno spazio vettoriale;
	\item assenza di assunzioni circa la struttura della matrice di affinità: è stato dimostrato che l'approccio funziona anche nel caso di funzioni di similarità asimmetriche o negative;
	\item non è necessaria alcuna conoscenza a priori circa il numero di clusters;
	\item permette l'estrazione di cluster sovrapposti.
\end{enumerate}

% subsection insiemi_dominanti_e_ottimi_locali (end)
% section insiemi_dominanti (end)
% chapter clustering (end)




















